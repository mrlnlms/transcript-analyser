#!/usr/bin/env python3
"""
üéØ TRANSCRIPT ANALYZER - CLI PRINCIPAL
üîß python run_analysis.py --project grupo_docentes

Interface de linha de comando para an√°lise automatizada de entrevistas.
"""

import argparse
import sys
import os
from pathlib import Path
from datetime import datetime
from typing import Optional, List

# Importar m√≥dulos do sistema
try:
    from config_loader import ConfigLoader, ResourceManager, ProjectConfig
    from engine.analyzer_core import TranscriptAnalyzer
    from engine.analysis_orchestrator import AnalysisOrchestrator
    from visuals.dashboard_generator import DashboardGenerator
except ImportError as e:
    print(f"‚ùå Erro ao importar m√≥dulos b√°sicos: {e}")
    print("üí° Execute primeiro: python setup.py para configurar o ambiente")
    sys.exit(1)

# Tentar importar sistema escal√°vel de visualiza√ß√µes
try:
    from visuals.visualization_manager import ScalableVisualizationManager
    SCALABLE_VISUALS = True
    print("‚úÖ Sistema escal√°vel de visualiza√ß√µes carregado")
except ImportError:
    SCALABLE_VISUALS = False
    print("‚ö†Ô∏è Sistema escal√°vel n√£o dispon√≠vel, usando sistema tradicional")


class AnalysisRunner:
    """üöÄ Orquestrador principal das an√°lises"""
    
    def __init__(self):
        self.config_loader = ConfigLoader()
        self.resource_manager = ResourceManager(self.config_loader)
        
    def run_single_analysis(self, project_name: str, file_path: Optional[str] = None):
        """üìä Executa an√°lise individual"""
        
        print(f"üéØ INICIANDO AN√ÅLISE: {project_name}")
        print("=" * 50)
        
        try:
            # 1. Carregar configura√ß√£o
            config = self.config_loader.load_project_config(project_name)
            print(f"‚úÖ Configura√ß√£o carregada: {config.project_name}")
            
            # 2. Inicializar analisador
            analyzer = AnalysisOrchestrator()
            
            # 3. Detectar arquivos para an√°lise
            project_dir = self.config_loader.projects_dir / project_name
            if file_path:
                files_to_analyze = [Path(file_path)]
            else:
                files_to_analyze = list((project_dir / "arquivos").glob("*.txt"))
            
            if not files_to_analyze:
                print("‚ùå Nenhum arquivo .txt encontrado em arquivos/")
                return False
            
            print(f"üìÅ Arquivos detectados: {len(files_to_analyze)}")
            
            # 4. Processar cada arquivo
            results = []
            for file_path in files_to_analyze:
                print(f"\nüîç Analisando: {file_path.name}")
                
                try:
                    result = analyzer.analyze_transcript(file_path, config.__dict__)

                    # DEBUG - Verificar dados dispon√≠veis
                    print("\nüîç DEBUG - Dados completos dispon√≠veis:")
                    print(f"  ‚úì temporal_analysis: {len(result.get('temporal_analysis', []))} pontos")
                    print(f"  ‚úì word_frequencies: {len(result.get('word_frequencies', {}))} palavras")
                    print(f"  ‚úì linguistic_patterns: {'‚úì' if result.get('linguistic_patterns') else '‚úó'}")
                    print(f"  ‚úì topic_hierarchy: {len(result.get('topic_hierarchy', {}).get('nodes', []))} n√≥s")
                    print(f"  ‚úì phases: {len(result.get('phases', {}))} fases")
                    print(f"  ‚úì contradictions: {len(result.get('contradictions', []))} contradi√ß√µes")
                    print("="*60)

                    result['filename'] = file_path.name
                    results.append(result)
                    print(f"‚úÖ {file_path.name} processado")
                    
                except Exception as e:
                    print(f"‚ùå Erro em {file_path.name}: {e}")
                    continue
            
            if not results:
                print("‚ùå Nenhuma an√°lise foi conclu√≠da com sucesso")
                return False
            
            # 5. Gerar visualiza√ß√µes
            if config.output['generate_visuals']:
                print(f"\nüìä Gerando visualiza√ß√µes...")
                
                for result in results:
                    output_dir = project_dir / "output" / result['filename'].replace('.txt', '')
                    output_dir.mkdir(parents=True, exist_ok=True)
                    
                    print(f"\nüé® Orquestrando visualiza√ß√µes para {result['filename']}...")
                    
                    # ChartOrchestrator - substitui TODO o c√≥digo hardcoded!
                    from visuals.chart_orchestrator import ChartOrchestrator
                    chart_orchestrator = ChartOrchestrator()
                    orchestration_result = chart_orchestrator.analyze(result, str(output_dir))
                    
                    print(f"üìä {orchestration_result['charts_created']}/{orchestration_result['charts_available']} gr√°ficos criados com sucesso!")
                    
                    # Mostrar gr√°ficos criados
                    for chart_info in orchestration_result['created_charts']:
                        print(f"  ‚úÖ {chart_info['chart']}")
                    
                    # Mostrar erros se houver
                    for error_info in orchestration_result['errors']:
                        print(f"  ‚ùå {error_info['chart']}: {error_info['error']}")
                
                print("‚úÖ Visualiza√ß√µes orquestradas geradas")
            
            # 6. Gerar relat√≥rios se habilitado
            if config.output['generate_markdown']:
                print(f"\nüìù Gerando relat√≥rios...")
                self._generate_markdown_reports(results, project_dir / "output")
            
            # 7. Resumo final
            self._print_analysis_summary(results, project_name)
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erro cr√≠tico na an√°lise: {e}")
            return False
    
    def _generate_visualizations_traditional(self, result, output_dir, config):
        """üìä Gera visualiza√ß√µes usando sistema tradicional"""
        try:
            dashboard = DashboardGenerator(config)
            dashboard.generate_complete_dashboard(result, output_dir)
            print("‚úÖ Visualiza√ß√µes tradicionais geradas")
        except Exception as e:
            print(f"‚ùå Erro ao gerar visualiza√ß√µes tradicionais: {e}")
    
    def run_comparative_analysis(self, project_names: List[str]):
        """üîÑ Executa an√°lise comparativa entre projetos"""
        
        print(f"üîÑ AN√ÅLISE COMPARATIVA")
        print(f"üìä Projetos: {', '.join(project_names)}")
        print("=" * 50)
        
        try:
            all_results = []
            
            # Coletar resultados de todos os projetos
            for project_name in project_names:
                config = self.config_loader.load_project_config(project_name)
                analyzer = AnalysisOrchestrator()
                
                project_dir = self.config_loader.projects_dir / project_name
                files = list((project_dir / "arquivos").glob("*.txt"))
                
                project_results = []
                for file_path in files:
                    result = analyzer.analyze_transcript(file_path, config.__dict__)
                    result['project'] = project_name
                    result['filename'] = file_path.name
                    project_results.append(result)
                
                all_results.extend(project_results)
                print(f"‚úÖ {project_name}: {len(project_results)} arquivos processados")
            
            # An√°lise comparativa
            from engine.comparative_analyzer import ComparativeAnalyzer
            
            comp_analyzer = ComparativeAnalyzer()
            comparison_results = comp_analyzer.compare_projects(all_results)
            
            # Gerar visualiza√ß√µes comparativas
            dashboard = DashboardGenerator(None)  # Usar config padr√£o para compara√ß√£o
            
            # Criar apenas a pasta comparisons se n√£o existir
            comparisons_dir = Path("projects/comparisons")
            comparisons_dir.mkdir(parents=True, exist_ok=True)

            # Gerar nome √∫nico para a imagem
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_filename = f"comparative_{timestamp}_test.png"
            output_path = comparisons_dir / output_filename
            
            dashboard.generate_comparative_dashboard(
                comparison_results,
                output_dir=str(comparisons_dir),
                output_path=str(output_path)
            )
            
            print(f"\n‚úÖ An√°lise comparativa conclu√≠da")
            print(f"üìÇ Resultados salvos em: {output_path}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erro na an√°lise comparativa: {e}")
            return False
    
    def _generate_markdown_reports(self, results: List[dict], output_dir: Path):
        """üìù Gera relat√≥rios em Markdown"""
        
        for result in results:
            # Criar caminho correto dentro da pasta do arquivo
            file_folder = result['filename'].replace('.txt', '')
            # output_dir j√° √© projects/nome/output/
            # Ent√£o: projects/nome/output/arquivo/arquivo.md
            report_path = output_dir / file_folder / f"_report_{file_folder}.md"
            
            # Garantir que a pasta existe (caso ainda n√£o tenha sido criada)
            report_path.parent.mkdir(parents=True, exist_ok=True)
            
            try:
                with open(report_path, 'w', encoding='utf-8') as f:
                    f.write(self._create_markdown_content(result))
                
                print(f"üìÑ Relat√≥rio gerado: {file_folder}.md")
            except Exception as e:
                print(f"‚ùå Erro ao gerar relat√≥rio {file_folder}.md: {e}")
    
    def _create_markdown_content(self, result: dict) -> str:
        """üìã Cria conte√∫do do relat√≥rio em Markdown"""
        
        content = f"""# An√°lise de Entrevista: {result['filename']}

**Data da An√°lise:** {datetime.now().strftime('%d/%m/%Y %H:%M')}

## üìä M√©tricas Globais

- **Sentimento Global:** {result['global_metrics']['global_sentiment']:.2f}
- **Coer√™ncia Tem√°tica:** {result['global_metrics']['thematic_coherence']:.2f}
- **Abertura Emocional:** {result['global_metrics']['emotional_openness']:.2f}

"""
        
        # Evolu√ß√£o Temporal
        if result.get('temporal_analysis'):
            content += "## üìà Evolu√ß√£o Temporal\n\n"
            phases = result.get('phases', {})
            for phase_name, phase_data in phases.items():
                if phase_data.get('sentiment_avg') is not None:
                    sentiment_emoji = "üòä" if phase_data['sentiment_avg'] > 0 else "üòê" if phase_data['sentiment_avg'] == 0 else "üòî"
                    content += f"- **{phase_name}**: {sentiment_emoji} Sentimento m√©dio: {phase_data['sentiment_avg']:.2f}\n"
            content += "\n"
        
        # Top 10 Palavras
        if result.get('word_frequencies'):
            content += "## üî§ Top 10 Palavras Mais Frequentes\n\n"
            for i, (word, freq) in enumerate(list(result['word_frequencies'].items())[:10], 1):
                content += f"{i}. **{word}**: {freq} vezes\n"
            content += "\n"
        
        # T√≥picos Principais
        content += "## üìà T√≥picos Principais\n\n"
        for i, topic in enumerate(result['topics'][:5]):
            distribution = result['topic_distribution'][i]
            content += f"### T√≥pico {i+1} ({distribution:.1%})\n"
            content += f"**Palavras-chave:** {', '.join(topic['words'][:8])}\n\n"
        
        # Rede de Conceitos
        if result.get('concept_network'):
            content += "## üï∏Ô∏è Principais Conex√µes entre Conceitos\n\n"
            for conn in result['concept_network'][:10]:
                content += f"- {conn['word1']} ‚Üî {conn['word2']} (for√ßa: {conn['weight']})\n"
            content += "\n"
        
        # An√°lise Lingu√≠stica
        content += "## üé≠ An√°lise Lingu√≠stica\n"
        
        linguistic = result.get('linguistic_patterns', {})
        
        # Para compatibilidade com estrutura antiga E nova
        if 'uncertainty_markers' in linguistic:
            uncertainty = linguistic.get('uncertainty_markers', {}).get('count', 0)
            certainty = linguistic.get('certainty_markers', {}).get('count', 0)
        else:
            uncertainty = linguistic.get('uncertainty_count', 0)
            certainty = linguistic.get('certainty_count', 0)
        
        content += f"- **Total de Hesita√ß√µes:** {linguistic.get('total_hesitations', 0)}\n"
        content += f"- **Marcadores de Incerteza:** {uncertainty}\n"
        content += f"- **Marcadores de Certeza:** {certainty}\n"
        
        if certainty > 0:
            ratio = uncertainty / certainty
            content += f"- **Raz√£o Incerteza/Certeza:** {ratio:.2f}\n"
        else:
            content += f"- **Raz√£o Incerteza/Certeza:** N/A\n"
            
        content += f"- **Complexidade M√©dia:** {linguistic.get('avg_sentence_length', 0):.1f} palavras/frase\n\n"
        
        # Padr√µes de Hesita√ß√£o
        if linguistic.get('hesitation_phrases'):
            content += "## üí¨ Padr√µes de Hesita√ß√£o\n\n"
            for word, count in sorted(linguistic['hesitation_phrases'].items(), key=lambda x: x[1], reverse=True)[:5]:
                content += f"- **{word}**: {count} ocorr√™ncias\n"
            content += "\n"
        
        # Contradi√ß√µes (se existirem)
        if result.get('contradictions') and len(result['contradictions']) > 0:
            content += "## ‚ö†Ô∏è Contradi√ß√µes Detectadas\n\n"
            for i, contradiction in enumerate(result['contradictions'][:3], 1):
                content += f"### Contradi√ß√£o {i} (Score: {contradiction.get('score', 0):.2f})\n"
                
                if 'text1' in contradiction and 'text2' in contradiction:
                    content += f"- **Trecho 1:** \"{contradiction['text1'][:80]}...\"\n"
                    content += f"- **Trecho 2:** \"{contradiction['text2'][:80]}...\"\n"
                
                if 'topics' in contradiction and contradiction['topics']:
                    topics_str = ', '.join(contradiction['topics'][:5]) if isinstance(contradiction['topics'], list) else str(contradiction['topics'])
                    content += f"- **T√≥picos relacionados:** {topics_str}\n"
                
                content += "\n"
        
        return content
    
    def _print_analysis_summary(self, results: List[dict], project_name: str):
        """üìã Imprime resumo da an√°lise"""
        
        print(f"\nüéØ RESUMO DA AN√ÅLISE: {project_name}")
        print("=" * 50)
        
        print(f"üìÅ Arquivos processados: {len(results)}")
        
        # M√©tricas m√©dias
        avg_sentiment = sum(r['global_metrics']['global_sentiment'] for r in results) / len(results)
        avg_coherence = sum(r['global_metrics']['thematic_coherence'] for r in results) / len(results)
        avg_openness = sum(r['global_metrics']['emotional_openness'] for r in results) / len(results)
        
        print(f"üòä Sentimento m√©dio: {avg_sentiment:+.2f}")
        print(f"üéØ Coer√™ncia m√©dia: {avg_coherence:.2f}")
        print(f"üí≠ Abertura m√©dia: {avg_openness:.2f}")
        
        # Arquivos com problemas
        problematic = [r for r in results if r['global_metrics']['thematic_coherence'] < 0.3]
        if problematic:
            print(f"\n‚ö†Ô∏è Arquivos com baixa coer√™ncia: {len(problematic)}")
            for r in problematic:
                print(f"   - {r['filename']}")
        
        print(f"\n‚úÖ An√°lise conclu√≠da com sucesso!")


def test_visualization_system():
    """üß™ Testa o sistema de visualiza√ß√µes"""
    
    print("üß™ TESTANDO SISTEMA DE VISUALIZA√á√ïES")
    print("=" * 50)
    
    if not SCALABLE_VISUALS:
        print("‚ùå Sistema escal√°vel n√£o dispon√≠vel para teste")
        return
    
    try:
        from visuals.visualization_manager import ScalableVisualizationManager
        
        # Criar gerenciador
        viz_manager = ScalableVisualizationManager()
        
        print(f"üé® Backends dispon√≠veis: {viz_manager.get_available_backends()}")
        
        # Dados de teste
        test_data = {
            'categories': ['A', 'B', 'C', 'D'],
            'values': [10, 25, 15, 30]
        }
        
        test_config = {
            'title': 'Teste de Gr√°fico de Barras',
            'output_path': 'test_bar_chart.png',
            'xlabel': 'Categorias',
            'ylabel': 'Valores'
        }
        
        # Testar cada backend
        for backend in viz_manager.get_available_backends():
            print(f"\nüîß Testando backend: {backend}")
            
            test_config['output_path'] = f'test_bar_chart_{backend}.png'
            if backend == 'plotly':
                test_config['output_path'] = f'test_bar_chart_{backend}.html'
            
            result = viz_manager.create_visualization('bar_chart', test_data, test_config, backend)
            
            if result:
                print(f"‚úÖ Sucesso: {result}")
            else:
                print(f"‚ùå Falha no backend {backend}")
        
        print("\n‚úÖ Teste conclu√≠do!")
        
    except Exception as e:
        print(f"‚ùå Erro no teste: {e}")


def main():
    """üéØ Fun√ß√£o principal do CLI"""
    
    parser = argparse.ArgumentParser(
        description="üéØ Transcript Analyzer - An√°lise automatizada de entrevistas qualitativas",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python run_analysis.py --project grupo_docentes
  python run_analysis.py --project professores_2024 --file entrevista1.txt
  python run_analysis.py --compare projeto1 projeto2 projeto3
  python run_analysis.py --test-visuals
        """
    )
    
    parser.add_argument(
        '--project', '-p',
        help="Nome do projeto para an√°lise"
    )
    
    parser.add_argument(
        '--file', '-f',
        help="Arquivo espec√≠fico para analisar (opcional)"
    )
    
    parser.add_argument(
        '--compare', '-c',
        nargs='+',
        help="Executar an√°lise comparativa entre projetos"
    )
    
    parser.add_argument(
        '--create-project',
        help="Criar novo projeto com estrutura padr√£o"
    )
    
    parser.add_argument(
        '--list-projects', '-l',
        action='store_true',
        help="Listar projetos dispon√≠veis"
    )
    
    parser.add_argument(
        '--setup',
        action='store_true',
        help="Configurar ambiente inicial"
    )
    
    parser.add_argument(
        '--test-visuals',
        action='store_true',
        help="Testar sistema de visualiza√ß√µes"
    )
    
    args = parser.parse_args()
    
    # Configurar ambiente se solicitado
    if args.setup:
        from config_loader import setup_environment
        setup_environment()
        return
    
    # Testar visualiza√ß√µes
    if args.test_visuals:
        test_visualization_system()
        return
    
    runner = AnalysisRunner()
    
    # Criar projeto
    if args.create_project:
        runner.config_loader.create_default_project(args.create_project)
        print(f"‚úÖ Projeto '{args.create_project}' criado")
        print(f"üìÅ Adicione arquivos .txt em: projects/{args.create_project}/arquivos/")
        return
    
    # Listar projetos
    if args.list_projects:
        projects_dir = runner.config_loader.projects_dir
        projects = [d.name for d in projects_dir.iterdir() if d.is_dir()]
        
        if projects:
            print("üìÅ Projetos dispon√≠veis:")
            for project in projects:
                config_file = projects_dir / project / "config_analise.json"
                status = "‚úÖ" if config_file.exists() else "‚ùå"
                print(f"   {status} {project}")
        else:
            print("‚ùå Nenhum projeto encontrado")
            print("üí° Use: --create-project nome_projeto")
        return
    
    # An√°lise comparativa
    if args.compare:
        success = runner.run_comparative_analysis(args.compare)
        sys.exit(0 if success else 1)
    
    # An√°lise individual
    if args.project:
        success = runner.run_single_analysis(args.project, args.file)
        sys.exit(0 if success else 1)
    
    # Se chegou aqui, nenhum comando foi especificado
    parser.print_help()


if __name__ == "__main__":
    main()