#!/usr/bin/env python3
"""
ğŸ¯ TRANSCRIPT ANALYZER - CLI PRINCIPAL
ğŸ”§ python run_analysis.py --project grupo_docentes

Interface de linha de comando para anÃ¡lise automatizada de entrevistas.
"""

import argparse
import sys
import os
from pathlib import Path
from datetime import datetime
from typing import Optional, List

# Importar mÃ³dulos do sistema
try:
    from config_loader import ConfigLoader, ResourceManager, ProjectConfig
    from engine.analyzer_core import TranscriptAnalyzer
    from engine.analysis_orchestrator import AnalysisOrchestrator
    from visuals.dashboard_generator import DashboardGenerator
except ImportError as e:
    print(f"âŒ Erro ao importar mÃ³dulos bÃ¡sicos: {e}")
    print("ğŸ’¡ Execute primeiro: python setup.py para configurar o ambiente")
    sys.exit(1)

# Tentar importar sistema escalÃ¡vel de visualizaÃ§Ãµes
try:
    from visuals.visualization_manager import ScalableVisualizationManager
    SCALABLE_VISUALS = True
    print("âœ… Sistema escalÃ¡vel de visualizaÃ§Ãµes carregado")
except ImportError:
    SCALABLE_VISUALS = False
    print("âš ï¸ Sistema escalÃ¡vel nÃ£o disponÃ­vel, usando sistema tradicional")


class AnalysisRunner:
    """ğŸš€ Orquestrador principal das anÃ¡lises"""
    
    def __init__(self):
        self.config_loader = ConfigLoader()
        self.resource_manager = ResourceManager(self.config_loader)
        
    def run_single_analysis(self, project_name: str, file_path: Optional[str] = None):
        """ğŸ“Š Executa anÃ¡lise individual"""
        
        print(f"ğŸ¯ INICIANDO ANÃLISE: {project_name}")
        print("=" * 50)
        
        try:
            # 1. Carregar configuraÃ§Ã£o
            config = self.config_loader.load_project_config(project_name)
            print(f"âœ… ConfiguraÃ§Ã£o carregada: {config.project_name}")
            
            # 2. Inicializar analisador
            analyzer = AnalysisOrchestrator()
            
            # 3. Detectar arquivos para anÃ¡lise
            project_dir = self.config_loader.projects_dir / project_name
            if file_path:
                files_to_analyze = [Path(file_path)]
            else:
                files_to_analyze = list((project_dir / "arquivos").glob("*.txt"))
            
            if not files_to_analyze:
                print("âŒ Nenhum arquivo .txt encontrado em arquivos/")
                return False
            
            print(f"ğŸ“ Arquivos detectados: {len(files_to_analyze)}")
            
            # 4. Processar cada arquivo
            results = []
            for file_path in files_to_analyze:
                print(f"\nğŸ” Analisando: {file_path.name}")
                
                try:
                    result = analyzer.analyze_transcript(file_path, config.__dict__)

                    # DEBUG - Verificar dados disponÃ­veis
                    print("\nğŸ” DEBUG - Dados completos disponÃ­veis:")
                    print(f"  âœ“ temporal_analysis: {len(result.get('temporal_analysis', []))} pontos")
                    print(f"  âœ“ word_frequencies: {len(result.get('word_frequencies', {}))} palavras")
                    print(f"  âœ“ linguistic_patterns: {'âœ“' if result.get('linguistic_patterns') else 'âœ—'}")
                    print(f"  âœ“ topic_hierarchy: {len(result.get('topic_hierarchy', {}).get('nodes', []))} nÃ³s")
                    print(f"  âœ“ phases: {len(result.get('phases', {}))} fases")
                    print(f"  âœ“ contradictions: {len(result.get('contradictions', []))} contradiÃ§Ãµes")
                    print("="*60)

                    result['filename'] = file_path.name
                    results.append(result)
                    print(f"âœ… {file_path.name} processado")
                    
                except Exception as e:
                    print(f"âŒ Erro em {file_path.name}: {e}")
                    continue
            
            if not results:
                print("âŒ Nenhuma anÃ¡lise foi concluÃ­da com sucesso")
                return False
            
            # 5. Gerar visualizaÃ§Ãµes
            if config.output['generate_visuals']:
                print(f"\nğŸ“Š Gerando visualizaÃ§Ãµes...")
                
                for result in results:
                    output_dir = project_dir / "output" / result['filename'].replace('.txt', '')
                    output_dir.mkdir(parents=True, exist_ok=True)
                    
                    print(f"\nğŸ¨ Orquestrando visualizaÃ§Ãµes para {result['filename']}...")
                    
                    # ChartOrchestrator - substitui TODO o cÃ³digo hardcoded!
                    from visuals.chart_orchestrator import ChartOrchestrator
                    chart_orchestrator = ChartOrchestrator()
                    orchestration_result = chart_orchestrator.analyze(result, str(output_dir))
                    
                    print(f"ğŸ“Š {orchestration_result['charts_created']}/{orchestration_result['charts_available']} grÃ¡ficos criados com sucesso!")
                    
                    # Mostrar grÃ¡ficos criados
                    for chart_info in orchestration_result['created_charts']:
                        print(f"  âœ… {chart_info['chart']}")
                    
                    # Mostrar erros se houver
                    for error_info in orchestration_result['errors']:
                        print(f"  âŒ {error_info['chart']}: {error_info['error']}")
                
                print("âœ… VisualizaÃ§Ãµes orquestradas geradas")
            
            # 6. Gerar relatÃ³rios se habilitado
            if config.output['generate_markdown']:
                print(f"\nğŸ“ Gerando relatÃ³rios...")
                self._generate_markdown_reports(results, project_dir / "output")
            
            # 7. Resumo final
            self._print_analysis_summary(results, project_name)
            
            return True
            
        except Exception as e:
            print(f"âŒ Erro crÃ­tico na anÃ¡lise: {e}")
            return False
    
    def _generate_visualizations_traditional(self, result, output_dir, config):
        """ğŸ“Š Gera visualizaÃ§Ãµes usando sistema tradicional"""
        try:
            dashboard = DashboardGenerator(config)
            dashboard.generate_complete_dashboard(result, output_dir)
            print("âœ… VisualizaÃ§Ãµes tradicionais geradas")
        except Exception as e:
            print(f"âŒ Erro ao gerar visualizaÃ§Ãµes tradicionais: {e}")
    
    def run_comparative_analysis(self, project_names: List[str]):
        """ğŸ”„ Executa anÃ¡lise comparativa entre projetos"""
        
        print(f"ğŸ”„ ANÃLISE COMPARATIVA")
        print(f"ğŸ“Š Projetos: {', '.join(project_names)}")
        print("=" * 50)
        
        try:
            all_results = []
            
            # Coletar resultados de todos os projetos
            for project_name in project_names:
                config = self.config_loader.load_project_config(project_name)
                analyzer = AnalysisOrchestrator()
                
                project_dir = self.config_loader.projects_dir / project_name
                files = list((project_dir / "arquivos").glob("*.txt"))
                
                project_results = []
                for file_path in files:
                    result = analyzer.analyze_transcript(file_path, config.__dict__)
                    result['project'] = project_name
                    result['filename'] = file_path.name
                    project_results.append(result)
                
                all_results.extend(project_results)
                print(f"âœ… {project_name}: {len(project_results)} arquivos processados")
            
            # AnÃ¡lise comparativa
            from engine.comparative_analyzer import ComparativeAnalyzer
            
            comp_analyzer = ComparativeAnalyzer()
            comparison_results = comp_analyzer.compare_projects(all_results)
            
            # Gerar visualizaÃ§Ãµes comparativas
            dashboard = DashboardGenerator(None)  # Usar config padrÃ£o para comparaÃ§Ã£o
            
            # Criar apenas a pasta comparisons se nÃ£o existir
            comparisons_dir = Path("projects/comparisons")
            comparisons_dir.mkdir(parents=True, exist_ok=True)

            # Gerar nome Ãºnico para a imagem
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_filename = f"comparative_{timestamp}_test.png"
            output_path = comparisons_dir / output_filename
            
            dashboard.generate_comparative_dashboard(
                comparison_results,
                output_dir=str(comparisons_dir),
                output_path=str(output_path)
            )
            
            print(f"\nâœ… AnÃ¡lise comparativa concluÃ­da")
            print(f"ğŸ“‚ Resultados salvos em: {output_path}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Erro na anÃ¡lise comparativa: {e}")
            return False
    
    def _generate_markdown_reports(self, results: List[dict], output_dir: Path):
        """ğŸ“ Gera relatÃ³rios em Markdown"""
        
        for result in results:
            # Criar caminho correto dentro da pasta do arquivo
            file_folder = result['filename'].replace('.txt', '')
            # output_dir jÃ¡ Ã© projects/nome/output/
            # EntÃ£o: projects/nome/output/arquivo/arquivo.md
            report_path = output_dir / file_folder / f"_report_{file_folder}.md"
            
            # Garantir que a pasta existe (caso ainda nÃ£o tenha sido criada)
            report_path.parent.mkdir(parents=True, exist_ok=True)
            
            try:
                with open(report_path, 'w', encoding='utf-8') as f:
                    f.write(self._create_markdown_content(result))
                
                print(f"ğŸ“„ RelatÃ³rio gerado: {file_folder}.md")
            except Exception as e:
                print(f"âŒ Erro ao gerar relatÃ³rio {file_folder}.md: {e}")
    
    def _create_markdown_content(self, result: dict) -> str:
        """ğŸ“‹ Cria conteÃºdo do relatÃ³rio em Markdown"""
        
        content = f"""# AnÃ¡lise de Entrevista: {result['filename']}

**Data da AnÃ¡lise:** {datetime.now().strftime('%d/%m/%Y %H:%M')}

## ğŸ“Š MÃ©tricas Globais

- **Sentimento Global:** {result['global_metrics']['global_sentiment']:.2f}
- **CoerÃªncia TemÃ¡tica:** {result['global_metrics']['thematic_coherence']:.2f}
- **Abertura Emocional:** {result['global_metrics']['emotional_openness']:.2f}

"""
        
        # EvoluÃ§Ã£o Temporal
        if result.get('temporal_analysis'):
            content += "## ğŸ“ˆ EvoluÃ§Ã£o Temporal\n\n"
            phases = result.get('phases', {})
            for phase_name, phase_data in phases.items():
                if phase_data.get('sentiment_avg') is not None:
                    sentiment_emoji = "ğŸ˜Š" if phase_data['sentiment_avg'] > 0 else "ğŸ˜" if phase_data['sentiment_avg'] == 0 else "ğŸ˜”"
                    content += f"- **{phase_name}**: {sentiment_emoji} Sentimento mÃ©dio: {phase_data['sentiment_avg']:.2f}\n"
            content += "\n"
        
        # Top 10 Palavras
        if result.get('word_frequencies'):
            content += "## ğŸ”¤ Top 10 Palavras Mais Frequentes\n\n"
            for i, (word, freq) in enumerate(list(result['word_frequencies'].items())[:10], 1):
                content += f"{i}. **{word}**: {freq} vezes\n"
            content += "\n"
        
        # TÃ³picos Principais
        content += "## ğŸ“ˆ TÃ³picos Principais\n\n"
        for i, topic in enumerate(result['topics'][:5]):
            distribution = result['topic_distribution'][i]
            content += f"### TÃ³pico {i+1} ({distribution:.1%})\n"
            content += f"**Palavras-chave:** {', '.join(topic['words'][:8])}\n\n"
        
        # Rede de Conceitos
        if result.get('concept_network'):
            content += "## ğŸ•¸ï¸ Principais ConexÃµes entre Conceitos\n\n"
            for conn in result['concept_network'][:10]:
                content += f"- {conn['word1']} â†” {conn['word2']} (forÃ§a: {conn['weight']})\n"
            content += "\n"
        
        # AnÃ¡lise LinguÃ­stica
        content += "## ğŸ­ AnÃ¡lise LinguÃ­stica\n"
        
        linguistic = result.get('linguistic_patterns', {})
        
        # Para compatibilidade com estrutura antiga E nova
        if 'uncertainty_markers' in linguistic:
            uncertainty = linguistic.get('uncertainty_markers', {}).get('count', 0)
            certainty = linguistic.get('certainty_markers', {}).get('count', 0)
        else:
            uncertainty = linguistic.get('uncertainty_count', 0)
            certainty = linguistic.get('certainty_count', 0)
        
        content += f"- **Total de HesitaÃ§Ãµes:** {linguistic.get('total_hesitations', 0)}\n"
        content += f"- **Marcadores de Incerteza:** {uncertainty}\n"
        content += f"- **Marcadores de Certeza:** {certainty}\n"
        
        if certainty > 0:
            ratio = uncertainty / certainty
            content += f"- **RazÃ£o Incerteza/Certeza:** {ratio:.2f}\n"
        else:
            content += f"- **RazÃ£o Incerteza/Certeza:** N/A\n"
            
        content += f"- **Complexidade MÃ©dia:** {linguistic.get('avg_sentence_length', 0):.1f} palavras/frase\n\n"
        
        # PadrÃµes de HesitaÃ§Ã£o
        if linguistic.get('hesitation_phrases'):
            content += "## ğŸ’¬ PadrÃµes de HesitaÃ§Ã£o\n\n"
            for word, count in sorted(linguistic['hesitation_phrases'].items(), key=lambda x: x[1], reverse=True)[:5]:
                content += f"- **{word}**: {count} ocorrÃªncias\n"
            content += "\n"
        
        # ContradiÃ§Ãµes (se existirem)
        if result.get('contradictions') and len(result['contradictions']) > 0:
            content += "## âš ï¸ ContradiÃ§Ãµes Detectadas\n\n"
            for i, contradiction in enumerate(result['contradictions'][:3], 1):
                content += f"### ContradiÃ§Ã£o {i} (Score: {contradiction.get('score', 0):.2f})\n"
                
                if 'text1' in contradiction and 'text2' in contradiction:
                    content += f"- **Trecho 1:** \"{contradiction['text1'][:80]}...\"\n"
                    content += f"- **Trecho 2:** \"{contradiction['text2'][:80]}...\"\n"
                
                if 'topics' in contradiction and contradiction['topics']:
                    topics_str = ', '.join(contradiction['topics'][:5]) if isinstance(contradiction['topics'], list) else str(contradiction['topics'])
                    content += f"- **TÃ³picos relacionados:** {topics_str}\n"
                
                content += "\n"
        
        return content
    
    def _print_analysis_summary(self, results: List[dict], project_name: str):
        """ğŸ“‹ Imprime resumo da anÃ¡lise"""
        
        print(f"\nğŸ¯ RESUMO DA ANÃLISE: {project_name}")
        print("=" * 50)
        
        print(f"ğŸ“ Arquivos processados: {len(results)}")
        
        # MÃ©tricas mÃ©dias
        avg_sentiment = sum(r['global_metrics']['global_sentiment'] for r in results) / len(results)
        avg_coherence = sum(r['global_metrics']['thematic_coherence'] for r in results) / len(results)
        avg_openness = sum(r['global_metrics']['emotional_openness'] for r in results) / len(results)
        
        print(f"ğŸ˜Š Sentimento mÃ©dio: {avg_sentiment:+.2f}")
        print(f"ğŸ¯ CoerÃªncia mÃ©dia: {avg_coherence:.2f}")
        print(f"ğŸ’­ Abertura mÃ©dia: {avg_openness:.2f}")
        
        # Arquivos com problemas
        problematic = [r for r in results if r['global_metrics']['thematic_coherence'] < 0.3]
        if problematic:
            print(f"\nâš ï¸ Arquivos com baixa coerÃªncia: {len(problematic)}")
            for r in problematic:
                print(f"   - {r['filename']}")
        
        print(f"\nâœ… AnÃ¡lise concluÃ­da com sucesso!")


def test_visualization_system():
    """ğŸ§ª Testa o sistema de visualizaÃ§Ãµes"""
    
    print("ğŸ§ª TESTANDO SISTEMA DE VISUALIZAÃ‡Ã•ES")
    print("=" * 50)
    
    if not SCALABLE_VISUALS:
        print("âŒ Sistema escalÃ¡vel nÃ£o disponÃ­vel para teste")
        return
    
    try:
        from visuals.visualization_manager import ScalableVisualizationManager
        
        # Criar gerenciador
        viz_manager = ScalableVisualizationManager()
        
        print(f"ğŸ¨ Backends disponÃ­veis: {viz_manager.get_available_backends()}")
        
        # Dados de teste
        test_data = {
            'categories': ['A', 'B', 'C', 'D'],
            'values': [10, 25, 15, 30]
        }
        
        test_config = {
            'title': 'Teste de GrÃ¡fico de Barras',
            'output_path': 'test_bar_chart.png',
            'xlabel': 'Categorias',
            'ylabel': 'Valores'
        }
        
        # Testar cada backend
        for backend in viz_manager.get_available_backends():
            print(f"\nğŸ”§ Testando backend: {backend}")
            
            test_config['output_path'] = f'test_bar_chart_{backend}.png'
            if backend == 'plotly':
                test_config['output_path'] = f'test_bar_chart_{backend}.html'
            
            result = viz_manager.create_visualization('bar_chart', test_data, test_config, backend)
            
            if result:
                print(f"âœ… Sucesso: {result}")
            else:
                print(f"âŒ Falha no backend {backend}")
        
        print("\nâœ… Teste concluÃ­do!")
        
    except Exception as e:
        print(f"âŒ Erro no teste: {e}")


def main():
    """ğŸ¯ FunÃ§Ã£o principal do CLI"""
    
    parser = argparse.ArgumentParser(
        description="ğŸ¯ Transcript Analyzer - AnÃ¡lise automatizada de entrevistas qualitativas",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python run_analysis.py --project grupo_docentes
  python run_analysis.py --project professores_2024 --file entrevista1.txt
  python run_analysis.py --compare projeto1 projeto2 projeto3
  python run_analysis.py --test-visuals
        """
    )
    
    parser.add_argument(
        '--project', '-p',
        help="Nome do projeto para anÃ¡lise"
    )
    
    parser.add_argument(
        '--file', '-f',
        help="Arquivo especÃ­fico para analisar (opcional)"
    )
    
    parser.add_argument(
        '--compare', '-c',
        nargs='+',
        help="Executar anÃ¡lise comparativa entre projetos"
    )
    
    parser.add_argument(
        '--create-project',
        help="Criar novo projeto com estrutura padrÃ£o"
    )
    
    parser.add_argument(
        '--list-projects', '-l',
        action='store_true',
        help="Listar projetos disponÃ­veis"
    )
    
    parser.add_argument(
        '--setup',
        action='store_true',
        help="Configurar ambiente inicial"
    )
    
    parser.add_argument(
        '--test-visuals',
        action='store_true',
        help="Testar sistema de visualizaÃ§Ãµes"
    )
    
    args = parser.parse_args()
    
    # Configurar ambiente se solicitado
    if args.setup:
        from config_loader import setup_environment
        setup_environment()
        return
    
    # Testar visualizaÃ§Ãµes
    if args.test_visuals:
        test_visualization_system()
        return
    
    runner = AnalysisRunner()
    
    # Criar projeto
    if args.create_project:
        runner.config_loader.create_default_project(args.create_project)
        print(f"âœ… Projeto '{args.create_project}' criado")
        print(f"ğŸ“ Adicione arquivos .txt em: projects/{args.create_project}/arquivos/")
        return
    
    # Listar projetos
    if args.list_projects:
        projects_dir = runner.config_loader.projects_dir
        projects = [d.name for d in projects_dir.iterdir() if d.is_dir()]
        
        if projects:
            print("ğŸ“ Projetos disponÃ­veis:")
            for project in projects:
                config_file = projects_dir / project / "config_analise.json"
                status = "âœ…" if config_file.exists() else "âŒ"
                print(f"   {status} {project}")
        else:
            print("âŒ Nenhum projeto encontrado")
            print("ğŸ’¡ Use: --create-project nome_projeto")
        return
    
    # AnÃ¡lise comparativa
    if args.compare:
        success = runner.run_comparative_analysis(args.compare)
        sys.exit(0 if success else 1)
    
    # AnÃ¡lise individual
    if args.project:
        success = runner.run_single_analysis(args.project, args.file)
        sys.exit(0 if success else 1)
    
    # Se chegou aqui, nenhum comando foi especificado
    parser.print_help()


if __name__ == "__main__":
    main()