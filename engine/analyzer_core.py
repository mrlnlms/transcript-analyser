"""üîç Engine de an√°lise principal"""

import json
import os
import numpy as np

from pathlib import Path
from typing import Dict, List, Any, Optional

from .analyzers.word_frequency import WordFrequencyAnalyzer

class TranscriptAnalyzer:
    """Analisador principal de transcri√ß√µes"""
    
    def __init__(self, config, resource_manager):
        self.config = config
        self.resources = resource_manager
        print(f"‚úÖ TranscriptAnalyzer inicializado para projeto: {config.project_name}")
    
    def analyze_transcript(self, file_path: Path) -> Dict[str, Any]:
        """An√°lise completa de uma transcri√ß√£o"""
        
        print(f"üîç Iniciando an√°lise de: {file_path.name}")
        
        try:
            # Ler arquivo
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            if not text.strip():
                raise ValueError("Arquivo vazio ou inv√°lido")
            
            # Calcular todas as an√°lises primeiro
            word_frequencies = self._count_word_frequencies(text)
            temporal_analysis = self._analyze_temporal(text)
            topics, topic_distribution, topic_hierarchy = self._extract_simple_topics(text, word_frequencies)
            
            # AN√ÅLISE REAL (b√°sica por enquanto)
            result = {
                'filename': file_path.name,
                'status': 'success',
                'global_metrics': self._calculate_global_metrics(text, temporal_analysis),
                'word_frequencies': word_frequencies,
                'temporal_analysis': temporal_analysis,
                'phases': self._extract_phases(temporal_analysis),
                'linguistic_patterns': self._detect_linguistic_patterns(text),
                'topics': topics,
                'topic_distribution': topic_distribution,
                'topic_hierarchy': topic_hierarchy,
                'contradictions': self._detect_contradictions(text, temporal_analysis),
                'concept_network': self._build_concept_network(text, word_frequencies)
            }
            
            print(f"‚úÖ An√°lise conclu√≠da: {file_path.name}")
            return result
            
        except Exception as e:
            print(f"‚ùå Erro na an√°lise de {file_path.name}: {e}")
            raise

    def _count_word_frequencies(self, text: str) -> Dict[str, int]:
        """Usa o novo sistema plug√°vel de an√°lise"""
        analyzer = WordFrequencyAnalyzer()
        result = analyzer.analyze(text)
        return result['word_frequencies']
    




    def _analyze_temporal(self, text: str) -> List[Dict[str, Any]]:
        """Divide texto em segmentos temporais e analisa evolu√ß√£o"""
        import re
        
        # Dividir em par√°grafos (ou senten√ßas grandes)
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        # Se muito poucos par√°grafos, dividir por senten√ßas
        if len(paragraphs) < 5:
            sentences = re.split(r'[.!?]+', text)
            # Agrupar senten√ßas em blocos de 3-4
            paragraphs = []
            for i in range(0, len(sentences), 3):
                block = ' '.join(sentences[i:i+3]).strip()
                if block:
                    paragraphs.append(block)
        
        # Limitar a 20 segmentos no m√°ximo
        if len(paragraphs) > 20:
            # Agrupar par√°grafos para ter ~20 segmentos
            step = len(paragraphs) // 20
            temp = []
            for i in range(0, len(paragraphs), step):
                temp.append(' '.join(paragraphs[i:i+step]))
            paragraphs = temp[:20]
        
        # Analisar cada segmento
        temporal_data = []
        for i, paragraph in enumerate(paragraphs):
            if not paragraph:
                continue
                
            # Calcular m√©tricas b√°sicas
            words = paragraph.split()
            
            # An√°lise de sentimento b√°sica com palavras-chave
            sentiment = 0.0
            paragraph_lower = paragraph.lower()
            
            # Palavras positivas
            positive_words = ['bom', '√≥timo', 'excelente', 'feliz', 'satisfeito', 'gosto', 
                            'adoro', 'maravilh', 'incr√≠vel', 'positiv', 'melhor', 'sucesso',
                            'consegui', 'aprendi', 'entendi', 'legal', 'bacana', 'top']
            
            # Palavras negativas  
            negative_words = ['ruim', 'p√©ssimo', 'triste', 'dif√≠cil', 'problema', 'erro',
                            'n√£o', 'nunca', 'medo', 'preocup', 'frustr', 'chato', 'cansado',
                            'complicado', 'confuso', 'd√∫vida']
            
            # Contar palavras positivas e negativas
            for word in positive_words:
                if word in paragraph_lower:
                    sentiment += 0.15
            
            for word in negative_words:
                if word in paragraph_lower:
                    sentiment -= 0.15
            
            # Ajustar por pontua√ß√£o
            if '!' in paragraph:
                sentiment += 0.1
            if '?' in paragraph and any(neg in paragraph_lower for neg in ['n√£o', 'como', 'por que']):
                sentiment -= 0.1
            
            # Varia√ß√£o para tornar mais interessante
            import random
            sentiment += random.uniform(-0.1, 0.1)
            
            # Detectar marcadores
            marker = "normal"
            if i == 0:
                marker = "INICIO"
            elif i == len(paragraphs) - 1:
                marker = "CONCLUSAO"
            elif "mas" in paragraph.lower() or "por√©m" in paragraph.lower():
                marker = "CONFLITO"
            elif "portanto" in paragraph.lower() or "assim" in paragraph.lower():
                marker = "SINTESE"
            
            temporal_data.append({
                'time': i * 5,  # Simular tempo em minutos
                'minute': i * 2.5,
                'sentiment': max(-1, min(1, sentiment)),  # Limitar entre -1 e 1
                'marker': marker,
                'phase': 'INICIO' if i < len(paragraphs) * 0.3 else 'DESENVOLVIMENTO' if i < len(paragraphs) * 0.7 else 'CONCLUSAO',
                'cognitive_load': len(words),
                'hesitations': paragraph.lower().count('n√©') + paragraph.lower().count('tipo')
            })
        
        return temporal_data
    

    def _extract_phases(self, temporal_data: List[Dict]) -> Dict[str, Dict]:
        """Extrai as fases da an√°lise temporal"""
        if not temporal_data:
            return {}
        
        phases = {
            'INICIO': {
                'start': 0,
                'end': 0,
                'sentiment_avg': 0,
                'duration_minutes': 0,
                'color': '#B3E5FC'
            },
            'DESENVOLVIMENTO': {
                'start': 0,
                'end': 0,
                'sentiment_avg': 0,
                'duration_minutes': 0,
                'color': '#4FC3F7'
            },
            'CONCLUSAO': {
                'start': 0,
                'end': 0,
                'sentiment_avg': 0,
                'duration_minutes': 0,
                'color': '#E1BEE7'
            }
        }
        
        # Calcular m√©tricas por fase
        for phase_name in phases:
            phase_data = [d for d in temporal_data if d.get('phase') == phase_name]
            if phase_data:
                phases[phase_name]['start'] = phase_data[0]['minute']
                phases[phase_name]['end'] = phase_data[-1]['minute']
                phases[phase_name]['duration_minutes'] = phases[phase_name]['end'] - phases[phase_name]['start']
                
                sentiments = [d['sentiment'] for d in phase_data]
                phases[phase_name]['sentiment_avg'] = sum(sentiments) / len(sentiments) if sentiments else 0
        
        return phases
    
    def _calculate_global_metrics(self, text: str, temporal_data: List[Dict]) -> Dict[str, float]:
        """Calcula m√©tricas globais do texto"""
        # Sentimento global (m√©dia dos segmentos)
        sentiments = [d['sentiment'] for d in temporal_data] if temporal_data else [0]
        global_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0
        
        # Vari√¢ncia do sentimento (abertura emocional)
        if len(sentiments) > 1:
            mean = sum(sentiments) / len(sentiments)
            variance = sum((s - mean) ** 2 for s in sentiments) / len(sentiments)
            sentiment_variance = variance
        else:
            sentiment_variance = 0.1
        
        # Hesita√ß√µes totais
        total_hesitations = text.lower().count('n√©') + text.lower().count('tipo') + \
                          text.lower().count('assim') + text.lower().count('ent√£o')
        
        # Coer√™ncia tem√°tica (baseada em repeti√ß√£o de palavras-chave)
        words = text.lower().split()
        unique_words = len(set(words))
        total_words = len(words)
        coherence = 1 - (unique_words / total_words) if total_words > 0 else 0.5
        
        return {
            'global_sentiment': round(global_sentiment, 2),
            'thematic_coherence': round(coherence, 2),
            'emotional_openness': round(1 + sentiment_variance, 2),
            'sentiment_variance': round(sentiment_variance, 2),
            'total_hesitations': total_hesitations
        }
    
    def _detect_linguistic_patterns(self, text: str) -> Dict[str, Any]:
        """Detecta padr√µes lingu√≠sticos reais no texto"""
        import re
        
        text_lower = text.lower()
        words = text_lower.split()
        
        # Marcadores de certeza
        certainty_phrases = [
            'com certeza', 'obviamente', 'claramente', 'sem d√∫vida', 
            'definitivamente', 'certamente', 'claro que', 'evidente',
            'tenho certeza', 'absolutamente', 'seguramente'
        ]
        
        # Marcadores de incerteza
        uncertainty_phrases = [
            'talvez', 'acho que', 'n√£o sei', 'pode ser', 'provavelmente',
            'me parece', 'acredito que', 'suponho', 'imagino que',
            'n√£o tenho certeza', 'possivelmente', 'quem sabe'
        ]
        
        # Hesita√ß√µes
        hesitation_words = ['n√©', 'tipo', 'assim', 'ent√£o', 'eh', 'ah', 'uhm', 'ahn']
        
        # Contar ocorr√™ncias
        certainty_count = sum(1 for phrase in certainty_phrases if phrase in text_lower)
        uncertainty_count = sum(1 for phrase in uncertainty_phrases if phrase in text_lower)
        
        # Hesita√ß√µes por palavra
        hesitations_by_word = {}
        total_hesitations = 0
        
        for word in hesitation_words:
            count = text_lower.count(word)
            if count > 0:
                hesitations_by_word[word] = {
                    'count': count,
                    'percentage': (count / len(words)) * 100 if words else 0
                }
                total_hesitations += count
        
        # Complexidade das frases
        sentences = re.split(r'[.!?]+', text)
        sentence_lengths = [len(s.split()) for s in sentences if s.strip()]
        avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0
        
        return {
            'certainty_markers': {
                'count': certainty_count,
                'examples': [p for p in certainty_phrases if p in text_lower][:5],
                'percentage': (certainty_count / len(sentences)) * 100 if sentences else 0
            },
            'uncertainty_markers': {
                'count': uncertainty_count,
                'examples': [p for p in uncertainty_phrases if p in text_lower][:5],
                'percentage': (uncertainty_count / len(sentences)) * 100 if sentences else 0,
                'ratio_to_certainty': uncertainty_count / certainty_count if certainty_count > 0 else uncertainty_count
            },
            'hesitations_by_word': hesitations_by_word,
            'hesitation_phrases': {word: text_lower.count(word) for word in hesitation_words if text_lower.count(word) > 0},
            'total_hesitations': total_hesitations,
            'avg_sentence_length': round(avg_sentence_length, 1),
            'sentence_length_std': round(np.std(sentence_lengths), 1) if len(sentence_lengths) > 1 else 0,
            'complexity_by_topic': {}  # TODO: implementar quando tivermos t√≥picos
        }
    
    def _build_concept_network(self, text: str, word_freq: Dict[str, int]) -> List[Dict[str, Any]]:
        """Constr√≥i rede de conceitos baseada em coocorr√™ncia"""
        import re
        from collections import defaultdict
        
        # Pegar top palavras mais frequentes
        top_words = list(word_freq.keys())[:30]
        
        # Dividir em senten√ßas
        sentences = re.split(r'[.!?]+', text.lower())
        
        # Contar coocorr√™ncias
        cooccurrence = defaultdict(int)
        
        for sentence in sentences:
            words_in_sentence = [w for w in sentence.split() if w in top_words]
            
            # Para cada par de palavras na senten√ßa
            for i, word1 in enumerate(words_in_sentence):
                for word2 in words_in_sentence[i+1:]:
                    if word1 != word2:
                        # Ordenar para evitar duplicatas (a,b) e (b,a)
                        pair = tuple(sorted([word1, word2]))
                        cooccurrence[pair] += 1
        
        # Criar lista de conex√µes
        connections = []
        for (word1, word2), weight in sorted(cooccurrence.items(), key=lambda x: x[1], reverse=True)[:20]:
            if weight > 1:  # Apenas conex√µes significativas
                connections.append({
                    'word1': word1,
                    'word2': word2,
                    'weight': weight
                })
        
        return connections
    


    def _extract_simple_topics(self, text: str, word_freq: Dict[str, int]) -> tuple:
        """Extrai t√≥picos simples baseados em agrupamento de palavras"""
        # Agrupar palavras por categorias tem√°ticas simples
        topic_keywords = {
            'Tecnologia': ['sistema', 'software', 'c√≥digo', 'programa', 'computador', 'dados', 'tecnologia', 'digital', 'internet', 'aplicativo'],
            'Educa√ß√£o': ['curso', 'aula', 'professor', 'aluno', 'escola', 'ensino', 'aprendizagem', 'estudo', 'educa√ß√£o', 'conhecimento'],
            'Trabalho': ['trabalho', 'empresa', 'projeto', 'equipe', 'cliente', 'processo', 'resultado', 'meta', 'objetivo', 'prazo'],
            'Pessoal': ['vida', 'fam√≠lia', 'casa', 'tempo', 'dia', 'pessoa', 'gente', 'amigo', 'momento', 'experi√™ncia'],
            'An√°lise': ['problema', 'solu√ß√£o', 'quest√£o', 'situa√ß√£o', 'caso', 'exemplo', 'forma', 'maneira', 'aspecto', 'ponto']
        }
        
        # Contar palavras por t√≥pico
        topic_scores = {}
        text_lower = text.lower()
        
        for topic, keywords in topic_keywords.items():
            score = sum(text_lower.count(keyword) for keyword in keywords)
            if score > 0:
                topic_scores[topic] = score
        
        # Se nenhum t√≥pico espec√≠fico, usar gen√©rico
        if not topic_scores:
            topic_scores['Geral'] = len(text.split())
        
        # Normalizar para distribui√ß√£o
        total = sum(topic_scores.values())
        topic_distribution = [score/total for score in topic_scores.values()]
        
        # Criar estrutura de t√≥picos
        topics = []
        for i, (topic_name, score) in enumerate(topic_scores.items()):
            # Pegar palavras relevantes do word_freq que se relacionam
            relevant_words = []
            for word in word_freq.keys():
                if any(keyword in word for keyword in topic_keywords.get(topic_name, [])):
                    relevant_words.append(word)
            
            # Se n√£o encontrou palavras relevantes, pegar as top do word_freq
            if not relevant_words:
                relevant_words = list(word_freq.keys())[i*8:(i+1)*8]
            
            topics.append({
                'id': f'topic_{i}',
                'words': relevant_words[:8],
                'weight': topic_distribution[i] if i < len(topic_distribution) else 0.1,
                'label': topic_name
            })
        
        # Criar hierarquia simples
        topic_hierarchy = {
            'central_theme': 'TEMAS PRINCIPAIS',
            'nodes': [{'id': 'central', 'label': 'TEMAS PRINCIPAIS', 'level': 0, 'size': 50}],
            'edges': []
        }
        
        for topic in topics:
            topic_hierarchy['nodes'].append({
                'id': topic['id'],
                'label': topic['label'],
                'level': 1,
                'size': int(topic['weight'] * 100)
            })
            topic_hierarchy['edges'].append({
                'source': 'central',
                'target': topic['id'],
                'weight': topic['weight']
            })
        
        return topics, topic_distribution, topic_hierarchy
    

    def _detect_contradictions(self, text: str, temporal_data: List[Dict]) -> List[Dict[str, Any]]:
        """Detecta poss√≠veis contradi√ß√µes no texto"""
        import re
        
        contradictions = []
        
        # Dividir em senten√ßas
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # Padr√µes de contradi√ß√£o comuns
        contradiction_patterns = [
            # Nega√ß√£o direta
            (r'\b(sempre|todos?|toda?s?|nunca|ningu√©m|nenhum)\b.*\b(mas|por√©m|entretanto|contudo)\b.*\b(√†s vezes|alguns?|alguma?s?|nem sempre)\b', 0.8),
            # Mudan√ßa de opini√£o
            (r'\b(acho|penso|acredito)\b.*\b(mas|por√©m)\b.*\b(n√£o sei|talvez n√£o|pensando bem)\b', 0.7),
            # Afirma√ß√£o seguida de nega√ß√£o
            (r'\b(sim|concordo|certo|verdade)\b.*\b(mas|por√©m|no entanto)\b.*\b(n√£o|errado|discordo)\b', 0.9),
            # Generaliza√ß√£o seguida de exce√ß√£o
            (r'\b(todos?|sempre|nunca)\b.*\b(exceto|menos|salvo|fora)\b', 0.6),
        ]
        
        # Buscar padr√µes em senten√ßas consecutivas
        for i in range(len(sentences) - 1):
            for j in range(i + 1, min(i + 5, len(sentences))):  # Olhar at√© 5 senten√ßas √† frente
                sent1 = sentences[i].lower()
                sent2 = sentences[j].lower()
                
                # Verificar padr√µes de contradi√ß√£o
                for pattern, base_score in contradiction_patterns:
                    combined = f"{sent1} {sent2}"
                    if re.search(pattern, combined):
                        contradictions.append({
                            'text1': sentences[i][:100],
                            'text2': sentences[j][:100],
                            'score': base_score,
                            'topics': self._extract_keywords(combined),
                            'timestamp1': i * 2.5,  # Estimativa temporal
                            'timestamp2': j * 2.5
                        })
                        break
                
                # Verificar nega√ß√µes do mesmo conceito
                # Palavras-chave importantes em ambas as senten√ßas
                words1 = set(re.findall(r'\b\w{4,}\b', sent1))
                words2 = set(re.findall(r'\b\w{4,}\b', sent2))
                common_words = words1.intersection(words2)
                
                if common_words and len(common_words) > 2:
                    # Verificar se uma nega a outra
                    has_negation1 = bool(re.search(r'\b(n√£o|nunca|nenhum|sem)\b', sent1))
                    has_negation2 = bool(re.search(r'\b(n√£o|nunca|nenhum|sem)\b', sent2))
                    
                    if has_negation1 != has_negation2:  # Uma tem nega√ß√£o, outra n√£o
                        # Variar o score baseado na quantidade de palavras em comum
                        score_variation = 0.5 + (len(common_words) * 0.05)
                        score_variation = min(score_variation, 0.85)  # Limitar m√°ximo
                        
                        contradictions.append({
                            'text1': sentences[i][:100],
                            'text2': sentences[j][:100],
                            'score': score_variation,
                            'topics': list(common_words)[:5],
                            'timestamp1': i * 2.5,
                            'timestamp2': j * 2.5
                        })
        
        # Detectar mudan√ßas de sentimento extremas
        if temporal_data and len(temporal_data) > 1:
            for i in range(len(temporal_data) - 1):
                current = temporal_data[i]
                next_seg = temporal_data[i + 1]
                
                # Mudan√ßa brusca de sentimento
                sentiment_change = abs(current['sentiment'] - next_seg['sentiment'])
                if sentiment_change > 1.0:  # Mudan√ßa significativa
                    contradictions.append({
                        'text1': f"Sentimento positivo no minuto {current['minute']:.1f}",
                        'text2': f"Sentimento negativo no minuto {next_seg['minute']:.1f}",
                        'score': min(sentiment_change / 2, 1.0),
                        'topics': ['mudan√ßa emocional', 'sentimento'],
                        'timestamp1': current['minute'],
                        'timestamp2': next_seg['minute']
                    })
        
        # Ordenar por score e retornar top contradi√ß√µes
        contradictions.sort(key=lambda x: x['score'], reverse=True)
        return contradictions[:5]  # M√°ximo 5 contradi√ß√µes
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Extrai palavras-chave de um texto"""
        import re
        
        # Palavras importantes (substantivos, verbos principais)
        words = re.findall(r'\b\w{4,}\b', text.lower())
        
        # Filtrar stopwords b√°sicas
        stopwords = {'para', 'mais', 'muito', 'mesmo', 'quando', 'onde', 'como', 'porque'}
        keywords = [w for w in words if w not in stopwords]
        
        # Retornar √∫nicas
        return list(set(keywords))[:5]