#!/bin/bash

echo "üîê REFATORA√á√ÉO SEGURA - Gerador de Markdown"
echo "==========================================="

# 1. Criar backup com timestamp
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="backups/refactor_$TIMESTAMP"

echo "üì¶ Criando backup em $BACKUP_DIR..."
mkdir -p "$BACKUP_DIR"

# Backup dos arquivos cr√≠ticos
cp run_analysis.py "$BACKUP_DIR/run_analysis.py.backup"
echo "‚úÖ Backup do run_analysis.py criado"

# 2. Salvar o novo m√≥dulo
echo "üíæ Criando markdown_generator.py..."
cat > markdown_generator.py << 'PYEOF'
#!/usr/bin/env python3
"""
Markdown Report Generator
Gera relat√≥rios em Markdown a partir dos resultados de an√°lise
"""

import logging
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

logger = logging.getLogger(__name__)


class MarkdownReportGenerator:
    """Gerador de relat√≥rios em Markdown com interpreta√ß√µes autom√°ticas"""
    
    def __init__(self):
        self.logger = logger
        
    def generate_report(self, result: Dict[str, Any], output_path: Path) -> str:
        """
        Gera relat√≥rio completo em Markdown
        
        Args:
            result: Resultados da an√°lise
            output_path: Caminho para salvar o relat√≥rio
            
        Returns:
            Caminho do arquivo gerado
        """
        try:
            self.logger.info("ÔøΩÔøΩ Gerando relat√≥rio Markdown...")
            
            # Criar conte√∫do
            content = self._create_content(result)
            
            # Salvar arquivo
            report_path = output_path / 'relatorio.md'
            report_path.write_text(content, encoding='utf-8')
            
            self.logger.info(f"‚úÖ Relat√≥rio salvo em: {report_path}")
            return str(report_path)
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao gerar relat√≥rio: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    def _create_content(self, result: Dict[str, Any]) -> str:
        """Cria conte√∫do completo do relat√≥rio"""
        sections = []
        
        # Cabe√ßalho
        sections.append(self._create_header(result))
        
        # Resumo executivo
        sections.append(self._create_executive_summary(result))
        
        # M√©tricas globais
        if 'global_metrics' in result:
            sections.append(self._create_global_metrics_section(result['global_metrics']))
        
        # An√°lise temporal
        if 'temporal_analysis' in result:
            sections.append(self._create_temporal_section(result['temporal_analysis']))
        
        # T√≥picos
        if 'topics' in result:
            sections.append(self._create_topics_section(result))
        
        # Rede conceitual
        if 'concept_network' in result:
            sections.append(self._create_network_section(result['concept_network']))
        
        # Padr√µes lingu√≠sticos
        if 'linguistic_patterns' in result:
            sections.append(self._create_patterns_section(result['linguistic_patterns']))
        
        # Contradi√ß√µes
        if 'contradictions' in result:
            sections.append(self._create_contradictions_section(result['contradictions']))
        
        # Palavras frequentes
        if 'word_frequencies' in result:
            sections.append(self._create_frequency_section(result['word_frequencies']))
        
        # Rodap√©
        sections.append(self._create_footer())
        
        return '\n\n'.join(filter(None, sections))
    
    def _create_header(self, result: Dict[str, Any]) -> str:
        """Cria cabe√ßalho do relat√≥rio"""
        filename = result.get('filename', 'arquivo')
        return f"""# üìä Relat√≥rio de An√°lise - {filename}

**Data**: {datetime.now().strftime('%d/%m/%Y %H:%M')}  
**Arquivo**: {filename}
"""
    
    def _create_executive_summary(self, result: Dict[str, Any]) -> str:
        """Cria resumo executivo com interpreta√ß√µes principais"""
        summary_parts = ["## üìã Resumo Executivo\n"]
        
        # Sentimento dominante
        if 'global_metrics' in result:
            sentiment = result['global_metrics'].get('global_sentiment', 0)
            if sentiment > 0.1:
                summary_parts.append("‚úÖ **Sentimento geral positivo** - A transcri√ß√£o apresenta tom otimista")
            elif sentiment < -0.1:
                summary_parts.append("‚ö†Ô∏è **Sentimento geral negativo** - Tons de preocupa√ß√£o ou cr√≠tica detectados")
            else:
                summary_parts.append("üîµ **Sentimento neutro** - Discurso equilibrado e objetivo")
        
        # T√≥picos principais
        if 'topics' in result and result['topics']:
            top_topic = result['topics'][0]
            summary_parts.append(f"üéØ **T√≥pico principal**: {top_topic.get('label', 'N/A')}")
        
        # Padr√µes not√°veis
        if 'linguistic_patterns' in result:
            patterns = result['linguistic_patterns']
            hesitations = patterns.get('total_hesitations', 0)
            if hesitations > 5:
                summary_parts.append("üí≠ **Alta hesita√ß√£o detectada** - Poss√≠vel incerteza ou reflex√£o profunda")
        
        return '\n'.join(summary_parts)
    
    def _create_global_metrics_section(self, metrics: Dict[str, Any]) -> str:
        """Se√ß√£o de m√©tricas globais"""
        return f"""## üìä M√©tricas Globais

- **Sentimento Global**: {metrics.get('global_sentiment', 0):.3f} {self._interpret_sentiment(metrics.get('global_sentiment', 0))}
- **Coer√™ncia Tem√°tica**: {metrics.get('thematic_coherence', 0):.2f} {self._interpret_coherence(metrics.get('thematic_coherence', 0))}
- **Abertura Emocional**: {metrics.get('emotional_openness', 0):.2f} {self._interpret_openness(metrics.get('emotional_openness', 0))}
"""
    
    def _create_temporal_section(self, temporal: List[Dict]) -> str:
        """Se√ß√£o de an√°lise temporal"""
        if not temporal:
            return ""
            
        content = ["## üìà An√°lise Temporal\n"]
        
        # Estat√≠sticas gerais
        sentiments = [seg.get('sentiment', 0) for seg in temporal]
        avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0
        
        content.append(f"**Sentimento m√©dio ao longo do tempo**: {avg_sentiment:.3f}")
        content.append(f"**Total de segmentos analisados**: {len(temporal)}")
        
        # Encontrar picos e vales
        if len(sentiments) > 2:
            max_idx = sentiments.index(max(sentiments))
            min_idx = sentiments.index(min(sentiments))
            content.append(f"\nüî∫ **Pico emocional**: Segmento {max_idx + 1} (sentimento: {sentiments[max_idx]:.3f})")
            content.append(f"üîª **Vale emocional**: Segmento {min_idx + 1} (sentimento: {sentiments[min_idx]:.3f})")
        
        return '\n'.join(content)
    
    def _create_topics_section(self, result: Dict[str, Any]) -> str:
        """Se√ß√£o de t√≥picos"""
        topics = result.get('topics', [])
        distribution = result.get('topic_distribution', [])
        
        if not topics:
            return ""
            
        content = ["## üéØ T√≥picos Identificados\n"]
        
        for i, topic in enumerate(topics[:5]):  # Top 5 t√≥picos
            label = topic.get('label', f'T√≥pico {i+1}')
            words = ', '.join(topic.get('words', [])[:5])
            
            if i < len(distribution):
                percentage = distribution[i] * 100
                content.append(f"### {label} ({percentage:.1f}%)")
            else:
                content.append(f"### {label}")
                
            content.append(f"**Palavras-chave**: {words}\n")
        
        return '\n'.join(content)
    
    def _create_network_section(self, network: List[Dict]) -> str:
        """Se√ß√£o de rede conceitual"""
        if not network:
            return ""
            
        content = ["## üï∏Ô∏è Rede de Conceitos\n"]
        
        # Ordenar por peso (corrigindo o erro de compara√ß√£o de dicts)
        try:
            # Filtrar apenas dicts v√°lidos com 'weight'
            valid_connections = [conn for conn in network if isinstance(conn, dict) and 'weight' in conn]
            
            if valid_connections:
                # Ordenar por peso
                sorted_network = sorted(valid_connections, key=lambda x: x.get('weight', 0), reverse=True)
                
                content.append("**Conex√µes mais fortes**:")
                for conn in sorted_network[:10]:  # Top 10 conex√µes
                    word1 = conn.get('word1', 'N/A')
                    word2 = conn.get('word2', 'N/A')
                    weight = conn.get('weight', 0)
                    content.append(f"- {word1} ‚ÜîÔ∏è {word2} (for√ßa: {weight})")
            else:
                content.append("*Nenhuma conex√£o v√°lida encontrada*")
                
        except Exception as e:
            self.logger.warning(f"Erro ao processar rede: {e}")
            content.append("*Erro ao processar conex√µes da rede*")
        
        return '\n'.join(content)
    
    def _create_patterns_section(self, patterns: Dict[str, Any]) -> str:
        """Se√ß√£o de padr√µes lingu√≠sticos"""
        content = ["## üí¨ Padr√µes Lingu√≠sticos\n"]
        
        # Hesita√ß√µes
        hesitations = patterns.get('hesitation_phrases', {})
        total_hesitations = patterns.get('total_hesitations', 0)
        
        if total_hesitations > 0:
            content.append(f"**Total de hesita√ß√µes**: {total_hesitations}")
            if isinstance(hesitations, dict) and hesitations:
                content.append("**Tipos de hesita√ß√£o**:")
                for phrase, count in list(hesitations.items())[:5]:
                    content.append(f"- '{phrase}': {count} vezes")
        
        # Certeza vs Incerteza
        uncertainty = patterns.get('uncertainty_markers', {}).get('count', 0)
        certainty = patterns.get('certainty_markers', {}).get('count', 0)
        
        if uncertainty + certainty > 0:
            certainty_ratio = certainty / (certainty + uncertainty) * 100
            content.append(f"\n**√çndice de certeza**: {certainty_ratio:.1f}%")
        
        # Comprimento m√©dio
        avg_length = patterns.get('avg_sentence_length', 0)
        if avg_length > 0:
            content.append(f"**Comprimento m√©dio das senten√ßas**: {avg_length:.1f} palavras")
        
        return '\n'.join(content)
    
    def _create_contradictions_section(self, contradictions: List[Dict]) -> str:
        """Se√ß√£o de contradi√ß√µes"""
        if not contradictions:
            return ""
            
        content = ["## ‚ö° Contradi√ß√µes Detectadas\n"]
        
        # Filtrar e ordenar contradi√ß√µes v√°lidas
        valid_contradictions = [c for c in contradictions if isinstance(c, dict) and 'score' in c]
        
        if valid_contradictions:
            sorted_contradictions = sorted(valid_contradictions, key=lambda x: x.get('score', 0), reverse=True)
            
            for i, contradiction in enumerate(sorted_contradictions[:5], 1):
                score = contradiction.get('score', 0)
                text1 = contradiction.get('text1', 'N/A')
                text2 = contradiction.get('text2', 'N/A')
                
                content.append(f"### Contradi√ß√£o {i} (confian√ßa: {score:.2f})")
                content.append(f"- **Afirma√ß√£o 1**: \"{text1}\"")
                content.append(f"- **Afirma√ß√£o 2**: \"{text2}\"\n")
        else:
            content.append("*Nenhuma contradi√ß√£o significativa detectada*")
        
        return '\n'.join(content)
    
    def _create_frequency_section(self, frequencies: Dict[str, int]) -> str:
        """Se√ß√£o de palavras frequentes"""
        if not frequencies:
            return ""
            
        content = ["## üìù Palavras Mais Frequentes\n"]
        
        # Ordenar por frequ√™ncia
        sorted_words = sorted(frequencies.items(), key=lambda x: x[1], reverse=True)
        
        for word, count in sorted_words[:15]:  # Top 15
            content.append(f"- **{word}**: {count} ocorr√™ncias")
        
        return '\n'.join(content)
    
    def _create_footer(self) -> str:
        """Rodap√© do relat√≥rio"""
        return """---

üìä *Relat√≥rio gerado automaticamente pelo Transcript Analyzer V2.0*  
üîç *Para an√°lises mais detalhadas, consulte as visualiza√ß√µes interativas*"""
    
    # M√©todos auxiliares de interpreta√ß√£o
    def _interpret_sentiment(self, value: float) -> str:
        """Interpreta valor de sentimento"""
        if value > 0.3:
            return "üòä Muito positivo"
        elif value > 0.1:
            return "üôÇ Positivo"
        elif value > -0.1:
            return "üòê Neutro"
        elif value > -0.3:
            return "üòï Negativo"
        else:
            return "üòî Muito negativo"
    
    def _interpret_coherence(self, value: float) -> str:
        """Interpreta coer√™ncia tem√°tica"""
        if value > 0.8:
            return "üéØ Excelente foco"
        elif value > 0.6:
            return "‚úÖ Boa estrutura"
        elif value > 0.4:
            return "üìä Estrutura razo√°vel"
        else:
            return "‚ö†Ô∏è Disperso"
    
    def _interpret_openness(self, value: float) -> str:
        """Interpreta abertura emocional"""
        if value > 2:
            return "üíñ Muito expressivo"
        elif value > 1:
            return "üí¨ Expressivo"
        elif value > 0.5:
            return "üîµ Moderado"
        else:
            return "üîí Reservado"
PYEOF

echo "‚úÖ markdown_generator.py criado"

# 3. Adicionar import no run_analysis.py
echo "üîß Adicionando import..."
LINE_NUM=$(grep -n "from pathlib import Path" run_analysis.py | head -1 | cut -d: -f1)
if [ ! -z "$LINE_NUM" ]; then
    sed -i.bak "${LINE_NUM}a\\
from markdown_generator import MarkdownReportGenerator" run_analysis.py
    echo "‚úÖ Import adicionado"
else
    echo "‚ö†Ô∏è  N√£o encontrei 'from pathlib import Path', adicionando import no in√≠cio..."
    sed -i.bak '1a\
from markdown_generator import MarkdownReportGenerator' run_analysis.py
fi

# 4. Localizar o m√©todo _generate_markdown_report
echo "üîç Localizando m√©todo _generate_markdown_report..."
START_LINE=$(grep -n "def _generate_markdown_report" run_analysis.py | cut -d: -f1)

if [ ! -z "$START_LINE" ]; then
    echo "‚úÖ M√©todo encontrado na linha $START_LINE"
    
    # Encontrar o pr√≥ximo m√©todo (para saber onde parar)
    NEXT_METHOD=$(sed -n "$((START_LINE+1)),\$p" run_analysis.py | grep -n "^[[:space:]]*def " | head -1 | cut -d: -f1)
    
    if [ ! -z "$NEXT_METHOD" ]; then
        END_LINE=$((START_LINE + NEXT_METHOD - 1))
    else
        END_LINE=$(wc -l < run_analysis.py)
    fi
    
    echo "üìù Substituindo m√©todo (linhas $START_LINE a $END_LINE)..."
    
    # Criar arquivo tempor√°rio com o novo m√©todo
    cat > temp_method.txt << 'METHODEOF'
    def _generate_markdown_report(self, result: Dict[str, Any], output_dir: Path) -> None:
        """Gera relat√≥rio Markdown usando o gerador modularizado"""
        try:
            generator = MarkdownReportGenerator()
            generator.generate_report(result, output_dir)
        except Exception as e:
            self.logger.error(f"Erro ao gerar relat√≥rio Markdown: {e}")
METHODEOF
    
    # Substituir o m√©todo
    sed -i.bak2 "${START_LINE},${END_LINE}d" run_analysis.py
    sed -i.bak3 "$((START_LINE-1))r temp_method.txt" run_analysis.py
    rm temp_method.txt
    
    echo "‚úÖ M√©todo _generate_markdown_report substitu√≠do"
fi

# 5. Remover m√©todo _create_markdown_content se existir
echo "üîç Procurando m√©todo _create_markdown_content..."
CREATE_LINE=$(grep -n "def _create_markdown_content" run_analysis.py | cut -d: -f1)

if [ ! -z "$CREATE_LINE" ]; then
    echo "‚úÖ M√©todo _create_markdown_content encontrado na linha $CREATE_LINE"
    
    # Encontrar o pr√≥ximo m√©todo
    NEXT_METHOD=$(sed -n "$((CREATE_LINE+1)),\$p" run_analysis.py | grep -n "^[[:space:]]*def " | head -1 | cut -d: -f1)
    
    if [ ! -z "$NEXT_METHOD" ]; then
        END_LINE=$((CREATE_LINE + NEXT_METHOD - 1))
    else
        END_LINE=$(wc -l < run_analysis.py)
    fi
    
    echo "üóëÔ∏è  Removendo m√©todo (linhas $CREATE_LINE a $END_LINE)..."
    sed -i.bak4 "${CREATE_LINE},${END_LINE}d" run_analysis.py
    echo "‚úÖ M√©todo _create_markdown_content removido"
else
    echo "‚ÑπÔ∏è  M√©todo _create_markdown_content n√£o encontrado (j√° foi removido?)"
fi

# 6. Limpar arquivos tempor√°rios de backup do sed
rm -f run_analysis.py.bak*

echo ""
echo "üéâ REFATORA√á√ÉO COMPLETA!"
echo "========================"
echo "‚úÖ Backup salvo em: $BACKUP_DIR"
echo "‚úÖ Novo m√≥dulo: markdown_generator.py"
echo "‚úÖ run_analysis.py refatorado"
echo ""
echo "üß™ Para testar:"
echo "   ./scripts/teste_automatico.sh"
echo ""
echo "üîÑ Para reverter se necess√°rio:"
echo "   cp $BACKUP_DIR/run_analysis.py.backup run_analysis.py"
echo "   rm markdown_generator.py"
